{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9507201,"sourceType":"datasetVersion","datasetId":5786596},{"sourceId":124694,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":104950,"modelId":129174}],"dockerImageVersionId":30777,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom skimage.io import imread\nfrom skimage.transform import resize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-02T00:19:20.166468Z","iopub.status.idle":"2024-10-02T00:19:20.166813Z","shell.execute_reply.started":"2024-10-02T00:19:20.166642Z","shell.execute_reply":"2024-10-02T00:19:20.166660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segmetation Model","metadata":{}},{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG16 \nfrom keras.regularizers import l2\n\n# # Detect and initialize the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect the TPU hardware\n# tf.tpu.experimental.initialize_tpu_system(tpu)  # Initialize the TPU system\n\n# # Instantiate the TPU distribution strategy\n# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n\n# with tpu_strategy.scope():\n\n\ndef load_data(images_path, masks_path):\n    images = []\n    masks = []\n\n    # Load images\n    for img_file in os.listdir(images_path):\n        img = imread(os.path.join(images_path, img_file))\n        img = resize(img, (224,224), mode='constant', preserve_range=True)\n        if img.ndim == 2:  # If the image is grayscale\n            img = np.stack((img,) * 3, axis=-1)  # Repeat the grayscale channel 3 times\n        images.append(img)\n\n    # Load masks\n    for mask_file in os.listdir(masks_path):\n        mask = imread(os.path.join(masks_path, mask_file))\n        mask = resize(mask, (224,224), mode='constant', preserve_range=True)\n        if mask.ndim == 2:  # If the mask is grayscale\n            mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n        masks.append(mask)\n\n    return np.array(images), np.array(masks)\n\n\ndef preprocess_data(images, masks):\n    images = images.astype('float32') / 255.0  # Normalize images\n    masks = masks.astype('float32') / 255.0  # Normalize masks\n    masks[masks > 0] = 1  # Convert to binary\n    return images, masks\n\n\ndef split_data(images, masks):\n    X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n    return X_train, X_val, y_train, y_val\n\n\n\ndef unet_with_backbone(input_shape=(224,224, 3)):\n    from tensorflow.keras import layers, models\n\n    # Load VGG16 without the top layer and with weights pre-trained on ImageNet\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n\n    # Encoder\n    encoder_output = base_model.output\n\n    # Bottleneck\n    bottleneck = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(encoder_output)\n    d5 = layers.Dropout(0.5)(bottleneck)  # Add Dropout\n\n    bottleneck = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(d5)\n\n    # Decoder\n    up1 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bottleneck)\n    up1 = layers.concatenate([up1, base_model.get_layer(\"block5_conv3\").output])  # Skip connection\n    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.001))(up1)\n    d4 = layers.Dropout(0.5)(conv4)  # Add Dropout\n\n    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.001))(d4)\n\n    up2 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4)\n    up2 = layers.concatenate([up2, base_model.get_layer(\"block4_conv3\").output])\n    conv5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.001))(up2)\n    d3 = layers.Dropout(0.5)(conv5)  # Add Dropout\n\n    conv5 = layers.Conv2D(256, (3, 3), activation='selu', padding='same')(d3)\n\n    up3 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)\n    up3 = layers.concatenate([up3, base_model.get_layer(\"block3_conv3\").output])\n    conv6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.001))(up3)\n    d1 = layers.Dropout(0.5)(conv6)  # Add Dropout\n\n    conv6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(d1)\n\n    up4 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)\n    up4 = layers.concatenate([up4, base_model.get_layer(\"block2_conv2\").output])\n    conv7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up4)\n    d2 = layers.Dropout(0.5)(conv7)  # Add Dropout\n    conv7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.001))(d2)\n\n    up5 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7)  # Final upsampling\n    conv8 = layers.Conv2D(32, (3, 3), activation='relu', padding='same',kernel_regularizer=l2(0.001))(up5)\n    d0 = layers.Dropout(0.5)(conv8)  # Add Dropout\n\n    conv8 = layers.Conv2D(32, (3, 3), activation='selu', padding='same',kernel_regularizer=l2(0.001))(d0)\n\n    # Final output layer\n    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(conv8)\n\n    model = models.Model(inputs=[base_model.input], outputs=[outputs])\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    return model\n\n\n\n\n\ndef train_model(model, X_train, y_train, X_val, y_val):\n    optimizer=Adam(learning_rate=0.00005)\n    \n    def dice_loss(y_true, y_pred, smooth=1e-6, weight_bg=0.2, weight_fg=0.8):\n        y_true_f = tf.keras.backend.flatten(y_true)\n        y_pred_f = tf.keras.backend.flatten(y_pred)\n\n        # Compute the intersection\n        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n\n        # Compute the Dice coefficient\n        dice = (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + \n                                               tf.keras.backend.sum(y_pred_f) + smooth)\n\n        return 1 - dice\n\n\n\n\n    # Compile the model\n    model.compile(optimizer=optimizer, \n                  loss=dice_loss, \n                  metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n\n    # Define callbacks for early stopping and model checkpointing\n    early_stopping = EarlyStopping(monitor='val_loss', \n                                   patience=10, \n                                   restore_best_weights=True,\n                                  verbose=1)\n\n    model_checkpoint = ModelCheckpoint('seg_model3.keras', \n                                        monitor='val_mean_io_u', \n                                        save_best_only=True,\n                                        mode='max',\n                                        verbose=1)\n    \n    #     from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    def custom_generator(image_generator, mask_generator):\n        while True:\n            image_batch = next(image_generator)   # Get the next batch of images\n            mask_batch = next(mask_generator)     # Get the next batch of masks\n\n            # Yield the tuple (image, mask) for training\n            yield (image_batch, mask_batch)\n\n    # Create two generators: one for images and one for masks\n    image_datagen = ImageDataGenerator(rotation_range=20, \n                                       width_shift_range=0.1,\n                                       height_shift_range=0.1, \n                                       shear_range=0.1,\n                                       zoom_range=0.1, \n                                       horizontal_flip=True, \n                                       fill_mode='nearest')\n\n    mask_datagen = ImageDataGenerator(rotation_range=20, \n                                      width_shift_range=0.1,\n                                      height_shift_range=0.1, \n                                      shear_range=0.1,\n                                      zoom_range=0.1, \n                                      horizontal_flip=True, \n                                      fill_mode='nearest')\n\n    # Assume you have your images and masks arrays\n    image_datagen.fit(X_train)\n    mask_datagen.fit(y_train)\n\n    # Create individual generators\n    image_generator = image_datagen.flow(X_train, batch_size=32, seed=42)\n    mask_generator = mask_datagen.flow(y_train, batch_size=32, seed=42)\n\n    # Create the custom generator\n    train_generator = custom_generator(image_generator, mask_generator)\n\n\n    # Train the model\n    history = model.fit(X_train,y_train, \n                        validation_data=(X_val, y_val), \n                        epochs=100, \n                        batch_size=8,\n                        steps_per_epoch=len(images) // 32, \n                        callbacks=[\n                                early_stopping, \n                                model_checkpoint])\n\n    return history\n\n\n# Set your paths\nimages_path = '/kaggle/input/brain-seg/images'\nmasks_path = '/kaggle/input/brain-seg/masks'\n\n# Load and preprocess data\nimages, masks = load_data(images_path, masks_path)\nimages, masks = preprocess_data(images, masks)\nX_train, X_val, y_train, y_val = split_data(images, masks)\n\n\n\n\n# Build, train, and evaluate model\nmodel = unet_with_backbone()\nhistory = train_model(model, X_train, y_train, X_val, y_val)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T00:19:20.178531Z","iopub.status.idle":"2024-10-02T00:19:20.178887Z","shell.execute_reply.started":"2024-10-02T00:19:20.178709Z","shell.execute_reply":"2024-10-02T00:19:20.178728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying mask on original Image","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# from keras.models import load_model\n# import matplotlib.pyplot as plt\n\n# # Define the custom loss function\n# def dice_loss(y_true, y_pred, smooth=1e-6, weight_bg=0.2, weight_fg=0.8):\n#         y_true_f = tf.keras.backend.flatten(y_true)\n#         y_pred_f = tf.keras.backend.flatten(y_pred)\n\n#         # Compute the intersection\n#         intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n\n#         # Compute the Dice coefficient\n#         dice = (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + \n#                                                tf.keras.backend.sum(y_pred_f) + smooth)\n\n#         return 1 - dice\n\n# # 1. Load the saved model with custom_objects\n# model = load_model('/kaggle/input/seg/tensorflow2/default/1/seg_model2.keras', custom_objects={'dice_loss': dice_loss})\n\n# # 2. Select the first 5 images and masks from your dataset\n# X_val_5 = X_val[:5]  # Assuming X_val is your validation set of images\n# y_val_5 = y_val[:5]  # Corresponding masks\n\n# # 3. Predict masks for the first 5 images\n# predicted_masks = model.predict(X_val_5)\n\n# # 4. Visualize the results\n# # Plot the first 5 images, their predicted masks, and the actual masks\n# for i in range(5):\n#     plt.figure(figsize=(12, 6))\n\n#     # Display the original image\n#     plt.subplot(1, 3, 1)\n#     plt.imshow(X_val_5[i], cmap='gray')\n#     plt.title('Original Image')\n\n#     # Display the predicted mask\n#     plt.subplot(1, 3, 2)\n#     plt.imshow(np.squeeze(predicted_masks[i]), cmap='gray')  # Squeeze to remove extra dimensions\n#     plt.title('Predicted Mask')\n\n#     # Display the actual mask\n#     plt.subplot(1, 3, 3)\n#     plt.imshow(np.squeeze(y_val_5[i]), cmap='gray')  # Squeeze to remove extra dimensions\n#     plt.title('Actual Mask')\n\n#     plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T00:19:20.180330Z","iopub.status.idle":"2024-10-02T00:19:20.180884Z","shell.execute_reply.started":"2024-10-02T00:19:20.180599Z","shell.execute_reply":"2024-10-02T00:19:20.180626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# def overlay_mask_on_image(image, predicted_mask, alpha=0.5):\n#     # Ensure the mask is binary\n#     binary_mask = np.squeeze(predicted_mask) > 0.5  # Thresholding if needed\n    \n    \n#     # Create a red overlay with the same shape as the original image\n#     red_overlay = np.zeros_like(image)  # Create an empty array for the red overlay\n#     red_overlay[binary_mask] = [255, 255, 0]  # Apply red color to the areas where the mask is True\n    \n#     # Blend the original image and the red overlay\n#     overlay_image = np.where(binary_mask[..., None], red_overlay, image)  # Use the mask to overlay red on the original image\n\n#     return overlay_image\n\n# # Usage\n\n# for i in range(0,4):\n#     predicted_mask = predicted_masks[i]  # Replace with your actual predicted mask\n#     overlayed_image = overlay_mask_on_image(X_val_5[i], predicted_mask)\n\n#     # Plot the results\n#     plt.figure(figsize=(5,5),)\n#     plt.imshow(overlayed_image)\n#     plt.axis('off')\n#     plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T00:19:20.182498Z","iopub.status.idle":"2024-10-02T00:19:20.183042Z","shell.execute_reply.started":"2024-10-02T00:19:20.182759Z","shell.execute_reply":"2024-10-02T00:19:20.182785Z"},"trusted":true},"execution_count":null,"outputs":[]}]}